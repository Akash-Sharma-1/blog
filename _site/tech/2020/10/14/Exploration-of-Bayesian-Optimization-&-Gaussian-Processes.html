<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Exploration of Bayesian Optimization and Gaussian Processes | Akash Sharma’s Blog</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Exploration of Bayesian Optimization and Gaussian Processes" />
<meta name="author" content="Akash Sharma" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset." />
<meta property="og:description" content="Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset." />
<link rel="canonical" href="http://localhost:4000/blog/tech/2020/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html" />
<meta property="og:url" content="http://localhost:4000/blog/tech/2020/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html" />
<meta property="og:site_name" content="Akash Sharma’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-14T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Exploration of Bayesian Optimization and Gaussian Processes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Akash Sharma"},"dateModified":"2020-10-14T00:00:00+05:30","datePublished":"2020-10-14T00:00:00+05:30","description":"Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset.","headline":"Exploration of Bayesian Optimization and Gaussian Processes","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/tech/2020/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html"},"url":"http://localhost:4000/blog/tech/2020/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html"}</script>
<!-- End Jekyll SEO tag -->

    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Akash Sharma&apos;s Blog" />
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W1GFNLSVY2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-W1GFNLSVY2');
</script>

    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
    <link rel="stylesheet" href="/blog/assets/css/main.css">
    <link rel="manifest" href="/blog/manifest.json">

    <!-- Support for Bootstrap https://getbootstrap.com -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js" integrity="sha256-BJeo0qm959uMBGb65z40ejJYGSgR7REI4+CW1fNKwOg=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.15.0/umd/popper.min.js" integrity="sha256-fTuUgtT7O2rqoImwjrhDgbXTKUwyxxujIMRIK7TbuNU=" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

    <!-- Support for pattern background https://github.com/btmills/geopattern -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js" integrity="sha256-AOyAcXCKLfI+FRdiZr4VTj9h9Wwv0YXXa2CPfUWbBS8=" crossorigin="anonymous"></script>

    <!-- Support for SVG icon https://iconify.design -->
    <script src="https://code.iconify.design/1/1.0.0/iconify.min.js"></script>

    <!-- Support for share and reaction btns -->
    <!-- <script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=5fae8b9ee3f5df0012a019ca&product=sop' async='async'></script> -->
    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5fae8ce8b9e345ad"></script>

</head>

    <body>
        <header>
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <nav class="container navbar navbar-expand-sm py-2">
        <a class="navbar-brand title p-0" href="/blog/">
          <!-- GITHUB ICON -->
          <!-- <span class="iconify" data-icon="octicon:mark-github"></span> -->
          <!-- Personal Icon -->
          <span class="material-icons" style="vertical-align:middle;font-size:inherit">font_download</span>
            <span style="vertical-align:middle">Akash Sharma's Blog</span>
        </a>

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <div class="navbar-nav ml-auto">
                
                  
                    <a class="nav-item nav-link text-right" href="/blog/">Home</a>
                  
                  

                
                  
                    <a class="nav-item nav-link text-right" href="/blog/classification/">Classification</a>
                  
                  

                
                  
                    <a class="nav-item nav-link text-right" href="/blog/leetcode/">Leetcode</a>
                  
                  

                
                  
                    <a class="nav-item nav-link text-right" href="/blog/about/">About Me</a>
                  
                  

                
                  
                  
                    <a class="nav-item nav-link text-right" href="https://akash-sharma-1.github.io/">Portfolio</a>
                  

                
            </div>
        </div>
    </nav>
</header>

<script>
    
        $(".navbar").addClass("navbar-light");
        $("a[href$='/blog/tech/2020/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html']").addClass("active");
    
</script>

        <div class="titlebar text-light">
    <div class="container">
        
            <!-- Post article title -->
            <div class="title pt-3">Exploration of Bayesian Optimization and Gaussian Processes</div>
            <div class="meta pb-3">
                <span class="pr-3">
                    
                        <span class="iconify" data-icon="octicon:calendar"></span>
                        <span>2020/10/14</span>
                    
                </span>

                
                    <span class="titlebar-info pr-3">
                        <span class="iconify" data-icon="octicon:tag"></span>
                        <a class="text-light" href="/blog/classification/#Machine Learning">Machine Learning</a>
                    </span>
                
                    <span class="titlebar-info pr-3">
                        <span class="iconify" data-icon="octicon:tag"></span>
                        <a class="text-light" href="/blog/classification/#Optimization">Optimization</a>
                    </span>
                
                    <span class="titlebar-info pr-3">
                        <span class="iconify" data-icon="octicon:tag"></span>
                        <a class="text-light" href="/blog/classification/#Probability">Probability</a>
                    </span>
                
                    <span class="titlebar-info pr-3">
                        <span class="iconify" data-icon="octicon:tag"></span>
                        <a class="text-light" href="/blog/classification/#Statistics">Statistics</a>
                    </span>
                

                
                    
                

            </div>
        
    </div>
</div>

<script>
    
        $(".titlebar").geopattern("Exploration of Bayesian Optimization and Gaussian Processes");
    
</script>

<div class="container">
    <div class="row my-2 my-md-4">
        <div class="content col-md-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/3.0.1/github-markdown.min.css" integrity="sha256-HbgiGHMLxHZ3kkAiixyvnaaZFNjNWLYKD/QG6PWaQPc=" crossorigin="anonymous" />

<!-- Support for deep anchor links https://github.com/bryanbraun/anchorjs -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js" integrity="sha256-0X7DxIkZMaHhAon0xCc/C/YhG6y0dg8Uj8c50+gbu8c=" crossorigin="anonymous"></script>

<!-- Support for fancybox https://github.com/fancyapps/fancybox -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

<!-- Support for MathJax https://github.com/mathjax/mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous" async></script>

<article class="markdown-body">
    <blockquote>
  <p>Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset.</p>
</blockquote>

<h2 id="introduction">Introduction</h2>

<p>Many optimization problems in machine learning are black box optimization problems where the objective function \(f(\mathbf{x})\) is a black box function<sup>[1][2]</sup>. We do not have an analytical expression for \(f\) nor do we know its derivatives. Evaluation of the function is restricted to sampling at a point \(\mathbf{x}\) and getting a possibly noisy response.</p>

<p>If \(f\) is cheap to evaluate we could sample at many points e.g. via grid search, random search or numeric gradient estimation. However, if function evaluation is expensive e.g. tuning hyperparameters of  a deep neural network, probe drilling for oil at given geographic coordinates or evaluating the effectiveness of a drug candidate taken from a chemical search space then it is important to minimize the number of samples drawn from the black box function \(f\).</p>

<p>This is the domain where Bayesian optimization techniques are most useful. They attempt to find the global optimimum in a minimum number of steps. Bayesian optimization incorporates prior belief about \(f\) and updates the prior with samples drawn from \(f\) to get a posterior that better approximates \(f\). The model used for approximating the objective function is called <em>surrogate model</em>. Bayesian optimization also uses an <em>acquisition function</em> that directs sampling to areas where an improvement over the current best observation is likely.</p>

<h3 id="surrogate-model">Surrogate model</h3>

<p>A popular surrogate model for Bayesian optimization are <a href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian processes</a> (GPs). I wrote about Gaussian processes in a <a href="/2018/03/19/gaussian-processes/">previous post</a>. If you are not familiar with GPs I recommend reading it first. GPs define a prior over functions and we can use them to incorporate prior beliefs about the objective function (smoothness, …). The GP posterior is cheap to evaluate and is used to propose points in the search space where sampling is likely to yield an improvement.</p>

<h3 id="acquisition-functions">Acquisition functions</h3>

<p>Proposing sampling points in the search space is done by acquisition functions. They trade off exploitation and exploration. Exploitation means sampling where the surrogate model predicts a high objective and exploration means sampling at locations where the prediction uncertainty is high. Both correspond to high acquisition function values and the goal is to maximize the acquisition function to determine the next sampling point.</p>

<p>More formally, the objective function \(f\) will be sampled at \(\mathbf{x}\_t = \operatorname{argmax}\_{\mathbf{x}} u(\mathbf{x} \lvert \mathcal{D}\_{1:t-1})\) where \(u\) is the acquisition function and \(\mathcal{D}\_{1:t-1} = \{(\mathbf{x}\_1, y_1),...,(\mathbf{x}\_{t-1}, y\_{t-1})\}\) are the \(t-1\) samples drawn from \(f\) so far. Popular acquisition functions are <em>maximum probability of improvement</em> (MPI), <em>expected improvement</em> (EI) and <em>upper confidence bound</em> (UCB)<sup>[1]</sup>. In the following, we will use the expected improvement (EI) which is most widely used and described further below.</p>

<h3 id="optimization-algorithm">Optimization algorithm</h3>

<p>The Bayesian optimization procedure is as follows. For \(t = 1,2,...\) repeat:</p>

<ul>
  <li>Find the next sampling point \(\mathbf{x}\_{t}\) by optimizing the acquisition function over the GP: \(\mathbf{x}\_t = \operatorname{argmax}\_{\mathbf{x}} u(\mathbf{x} \lvert \mathcal{D}\_{1:t-1})\)</li>
  <li>Obtain a possibly noisy sample \(y_t = f(\mathbf{x}_t) + \epsilon_t\) from the objective function \(f\).</li>
  <li>Add the sample to previous samples \(\mathcal{D}\_{1:t} = \{\mathcal{D}\_{1:t-1}, (\mathbf{x}\_t,y\_t)\}\) and update the GP.</li>
</ul>

<h3 id="expected-improvement">Expected improvement</h3>

<p>Expected improvement is defined as</p>

\[\operatorname{EI}(\mathbf{x}) = \mathbb{E}\max(f(\mathbf{x}) - f(\mathbf{x}^+), 0)\tag{1}\]

<p>where \(f(\mathbf{x}^+)\) is the value of the best sample so far and \(\mathbf{x}^+\) is the location of that sample i.e. \(\mathbf{x}^+ = \operatorname{argmax}\_{\mathbf{x}\_i \in \mathbf{x}\_{1:t}} f(\mathbf{x}\_i)\). The expected improvement can be evaluated analytically under the GP model<sup>[3]</sup>:</p>

\[\operatorname{EI}(\mathbf{x}) =
\begin{cases}
(\mu(\mathbf{x}) - f(\mathbf{x}^+) - \xi)\Phi(Z) + \sigma(\mathbf{x})\phi(Z)  &amp;\text{if}\ \sigma(\mathbf{x}) &gt; 0 \\
0 &amp; \text{if}\ \sigma(\mathbf{x}) = 0
\end{cases}\tag{2}\]

<p>where</p>

\[Z =
\begin{cases}
\frac{\mu(\mathbf{x}) - f(\mathbf{x}^+) - \xi}{\sigma(\mathbf{x})} &amp;\text{if}\ \sigma(\mathbf{x}) &gt; 0 \\
0 &amp; \text{if}\ \sigma(\mathbf{x}) = 0
\end{cases}\]

<p>where \(\mu(\mathbf{x})\) and \(\sigma(\mathbf{x})\) are the mean and the standard deviation of the GP posterior predictive at \(\mathbf{x}\), respectively. \(\Phi\) and \(\phi\) are the CDF and PDF of the standard normal distribution, respectively. The first summation term in Equation (2) is the exploitation term and second summation term is the exploration term.</p>

<p>Parameter \(\xi\) in Equation (2) determines the amount of exploration during optimization and higher \(\xi\) values lead to more exploration. In other words, with increasing \(\xi\) values, the importance of improvements predicted by the GP posterior mean \(\mu(\mathbf{x})\) decreases relative to the importance of potential improvements in regions of high prediction uncertainty, represented by large \(\sigma(\mathbf{x})\) values. A recommended default value for \(\xi\) is \(0.01\).</p>

<p>With this minimum of theory we can start implementing Bayesian optimization. The next section shows a basic implementation with plain NumPy and SciPy, later sections demonstrate how to use existing libraries. Finally, Bayesian optimization is used to tune the hyperparameters of a tree-based regression model.</p>

<h2 id="implementation-with-numpy-and-scipy">Implementation with NumPy and SciPy</h2>

<p>In this section, we will implement the acquisition function and its optimization in plain NumPy and SciPy and use scikit-learn for the Gaussian process implementation. Although we have an analytical expression of the optimization objective <code class="language-plaintext highlighter-rouge">f</code> in the following example, we treat is as black box and iteratively approximate it with a Gaussian process during Bayesian optimization. Furthermore, samples drawn from the objective function are noisy and the noise level is given by the <code class="language-plaintext highlighter-rouge">noise</code> variable. Optimization is done within given <code class="language-plaintext highlighter-rouge">bounds</code>. We also assume that there exist two initial samples in <code class="language-plaintext highlighter-rouge">X_init</code> and <code class="language-plaintext highlighter-rouge">Y_init</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.7</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">X_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">]])</span>
<span class="n">Y_init</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X_init</span><span class="p">)</span>
</code></pre></div></div>

<p>The following plot shows the noise-free objective function, the amount of noise by plotting a large number of samples and the two initial samples.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Dense grid of points within bounds
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="mf">0.01</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Noise-free objective function values at X
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plot optimization objective with noise level
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="s">'y--'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Noise-free objective'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s">'bx'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Noisy samples'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_init</span><span class="p">,</span> <span class="n">Y_init</span><span class="p">,</span> <span class="s">'kx'</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Initial samples'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="/blog/img/2018-03-21/output_4_0.png" alt="png" /></p>

<p>Goal is to find the global optimum on the left in a small number of steps. The next step is to implement the acquisition function defined in Equation (2) as <code class="language-plaintext highlighter-rouge">expected_improvement</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">expected_improvement</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="s">'''
    Computes the EI at points X based on existing samples X_sample
    and Y_sample using a Gaussian process surrogate model.

    Args:
        X: Points at which EI shall be computed (m x d).
        X_sample: Sample locations (n x d).
        Y_sample: Sample values (n x 1).
        gpr: A GaussianProcessRegressor fitted to samples.
        xi: Exploitation-exploration trade-off parameter.

    Returns:
        Expected improvements at points X.
    '''</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">gpr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">mu_sample</span> <span class="o">=</span> <span class="n">gpr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>

    <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Needed for noise-based model,
</span>    <span class="c1"># otherwise use np.max(Y_sample).
</span>    <span class="c1"># See also section 2.4 in [...]
</span>    <span class="n">mu_sample_opt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">mu_sample</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">np</span><span class="p">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s">'warn'</span><span class="p">):</span>
        <span class="n">imp</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">mu_sample_opt</span> <span class="o">-</span> <span class="n">xi</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">imp</span> <span class="o">/</span> <span class="n">sigma</span>
        <span class="n">ei</span> <span class="o">=</span> <span class="n">imp</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        <span class="n">ei</span><span class="p">[</span><span class="n">sigma</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">return</span> <span class="n">ei</span>
</code></pre></div></div>

<p>We also need a function that proposes the next sampling point by computing the location of the acquisition function maximum. Optimization is restarted <code class="language-plaintext highlighter-rouge">n_restarts</code> times to avoid local optima.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="k">def</span> <span class="nf">propose_location</span><span class="p">(</span><span class="n">acquisition</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">n_restarts</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
    <span class="s">'''
    Proposes the next sampling point by optimizing the acquisition function.

    Args:
        acquisition: Acquisition function.
        X_sample: Sample locations (n x d).
        Y_sample: Sample values (n x 1).
        gpr: A GaussianProcessRegressor fitted to samples.

    Returns:
        Location of the acquisition function maximum.
    '''</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">X_sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">min_x</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">min_obj</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="c1"># Minimization objective is the negative acquisition function
</span>        <span class="k">return</span> <span class="o">-</span><span class="n">acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">)</span>

    <span class="c1"># Find the best optimum by starting from n_restart different random points.
</span>    <span class="k">for</span> <span class="n">x0</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_restarts</span><span class="p">,</span> <span class="n">dim</span><span class="p">)):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">min_obj</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'L-BFGS-B'</span><span class="p">)</span>        
        <span class="k">if</span> <span class="n">res</span><span class="p">.</span><span class="n">fun</span> <span class="o">&lt;</span> <span class="n">min_val</span><span class="p">:</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">fun</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">min_x</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">x</span>           

    <span class="k">return</span> <span class="n">min_x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we have all components needed to run Bayesian optimization with the <a href="#Optimization-algorithm">algorithm</a> outlined above. The Gaussian process in the following example is configured with a <a href="http://scikit-learn.org/stable/modules/gaussian_process.html#matern-kernel">Matérn kernel</a> which is a generalization of the squared exponential kernel or RBF kernel. The known noise level is configured with the <code class="language-plaintext highlighter-rouge">alpha</code> parameter.</p>

<p>Bayesian optimization runs for 10 iterations. In each iteration, a row with two plots is produced. The left plot shows the noise-free objective function, the surrogate function which is the GP posterior predictive mean, the 95% confidence interval of the mean and the noisy samples obtained from the objective function so far. The right plot shows the acquisition function. The vertical dashed line in both plots shows the proposed sampling point for the next iteration which corresponds to the maximum of the acquisition function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ConstantKernel</span><span class="p">,</span> <span class="n">Matern</span>
<span class="kn">from</span> <span class="nn">bayesian_optimization_util</span> <span class="kn">import</span> <span class="n">plot_approximation</span><span class="p">,</span> <span class="n">plot_acquisition</span>

<span class="c1"># Gaussian process with Mat??rn kernel as surrogate model
</span><span class="n">m52</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">Matern</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">gpr</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Initialize samples
</span><span class="n">X_sample</span> <span class="o">=</span> <span class="n">X_init</span>
<span class="n">Y_sample</span> <span class="o">=</span> <span class="n">Y_init</span>

<span class="c1"># Number of iterations
</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="c1"># Update Gaussian process with existing samples
</span>    <span class="n">gpr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">)</span>

    <span class="c1"># Obtain next sampling point from the acquisition function (expected_improvement)
</span>    <span class="n">X_next</span> <span class="o">=</span> <span class="n">propose_location</span><span class="p">(</span><span class="n">expected_improvement</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>

    <span class="c1"># Obtain next noisy sample from the objective function
</span>    <span class="n">Y_next</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X_next</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

    <span class="c1"># Plot samples, surrogate function, noise-free objective and next sampling location
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plot_approximation</span><span class="p">(</span><span class="n">gpr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Iteration </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plot_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">expected_improvement</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">),</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Add sample to previous samples
</span>    <span class="n">X_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">X_next</span><span class="p">))</span>
    <span class="n">Y_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">Y_sample</span><span class="p">,</span> <span class="n">Y_next</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/blog/img/2018-03-21/output_10_0.png" alt="png" /></p>

<p>Note how the two initial samples initially drive search into the direction of the local maximum on the right side but exploration allows the algorithm to escape from that local optimum and find the global optimum on the left side. Also note how sampling point proposals often fall within regions of high uncertainty (exploration) and are not only driven by the highest surrogate function values (exploitation).</p>

<p>A convergence plot reveals how many iterations are needed the find a maximum and if the sampling point proposals stay around that maximum i.e. converge to small proposal differences between consecutive steps.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">bayesian_optimization_util</span> <span class="kn">import</span> <span class="n">plot_convergence</span>

<span class="n">plot_convergence</span><span class="p">(</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/img/2018-03-21/output_12_0.png" alt="png" /></p>

<h2 id="bayesian-optimization-libraries">Bayesian optimization libraries</h2>

<p>There are numerous Bayesian optimization libraries out there and giving a comprehensive overview is not the goal of this article. Instead, I’ll pick two that I used in the past and show the minimum setup needed to get the previous example running.</p>

<h3 id="scikit-optimize">Scikit-optimize</h3>

<p><a href="https://scikit-optimize.github.io/">Scikit-optimize</a> is a library for sequential model-based optimization that is based on <a href="http://scikit-learn.org/">scikit-learn</a>. It also supports Bayesian optimization using Gaussian processes. The API is designed around minimization, hence, we have to provide negative objective function values.  The results obtained here slightly differ from previous results because of non-deterministic optimization behavior and different noisy samples drawn from the objective function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">skopt</span> <span class="kn">import</span> <span class="n">gp_minimize</span>
<span class="kn">from</span> <span class="nn">skopt.learning</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="nn">skopt.learning.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ConstantKernel</span><span class="p">,</span> <span class="n">Matern</span>

<span class="c1"># Use custom kernel and estimator to match previous example
</span><span class="n">m52</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">Matern</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">gpr</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">gp_minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">))[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">bounds</span><span class="p">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">base_estimator</span><span class="o">=</span><span class="n">gpr</span><span class="p">,</span>
                <span class="n">acq_func</span><span class="o">=</span><span class="s">'EI'</span><span class="p">,</span>      <span class="c1"># expected improvement
</span>                <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>            <span class="c1"># exploitation-exploration trade-off
</span>                <span class="n">n_calls</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>         <span class="c1"># number of iterations
</span>                <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># initial samples are provided
</span>                <span class="n">x0</span><span class="o">=</span><span class="n">X_init</span><span class="p">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="c1"># initial samples
</span>                <span class="n">y0</span><span class="o">=-</span><span class="n">Y_init</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>

<span class="c1"># Fit GP model to samples for plotting results
</span><span class="n">gpr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">x_iters</span><span class="p">,</span> <span class="o">-</span><span class="n">r</span><span class="p">.</span><span class="n">func_vals</span><span class="p">)</span>

<span class="c1"># Plot the fitted model and the noisy samples
</span><span class="n">plot_approximation</span><span class="p">(</span><span class="n">gpr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">r</span><span class="p">.</span><span class="n">x_iters</span><span class="p">,</span> <span class="o">-</span><span class="n">r</span><span class="p">.</span><span class="n">func_vals</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/img/2018-03-21/output_14_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_convergence</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">x_iters</span><span class="p">),</span> <span class="o">-</span><span class="n">r</span><span class="p">.</span><span class="n">func_vals</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/img/2018-03-21/output_15_0.png" alt="png" /></p>

<h2 id="gpyopt">GPyOpt</h2>

<p><a href="http://sheffieldml.github.io/GPyOpt/">GPyOpt</a> is a Bayesian optimization library based on <a href="https://sheffieldml.github.io/GPy/">GPy</a>. The abstraction level of the API is comparable to that of scikit-optimize. The <code class="language-plaintext highlighter-rouge">BayesianOptimization</code> API provides a <code class="language-plaintext highlighter-rouge">maximize</code> parameter to configure whether the objective function shall be maximized or minimized (default). In version 1.2.1, this seems to be ignored when providing initial samples, so we have to negate their target values manually in the following example. Also, the built-in <code class="language-plaintext highlighter-rouge">plot_acquisition</code> and <code class="language-plaintext highlighter-rouge">plot_convergence</code> methods display the minimization result in any case. Again, the results obtained here slightly differ from previous results because of non-deterministic optimization behavior and different noisy samples drawn from the objective function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">GPy</span>
<span class="kn">import</span> <span class="nn">GPyOpt</span>

<span class="kn">from</span> <span class="nn">GPyOpt.methods</span> <span class="kn">import</span> <span class="n">BayesianOptimization</span>

<span class="n">kernel</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Matern52</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">bds</span> <span class="o">=</span> <span class="p">[{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'X'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="n">bounds</span><span class="p">.</span><span class="n">ravel</span><span class="p">()}]</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span>
                                 <span class="n">domain</span><span class="o">=</span><span class="n">bds</span><span class="p">,</span>
                                 <span class="n">model_type</span><span class="o">=</span><span class="s">'GP'</span><span class="p">,</span>
                                 <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span>
                                 <span class="n">acquisition_type</span> <span class="o">=</span><span class="s">'EI'</span><span class="p">,</span>
                                 <span class="n">acquisition_jitter</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
                                 <span class="n">X</span><span class="o">=</span><span class="n">X_init</span><span class="p">,</span>
                                 <span class="n">Y</span><span class="o">=-</span><span class="n">Y_init</span><span class="p">,</span>
                                 <span class="n">noise_var</span> <span class="o">=</span> <span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                                 <span class="n">exact_feval</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                 <span class="n">normalize_Y</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                 <span class="n">maximize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">optimizer</span><span class="p">.</span><span class="n">run_optimization</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">plot_acquisition</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/img/2018-03-21/output_17_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="p">.</span><span class="n">plot_convergence</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/img/2018-03-21/output_18_0.png" alt="png" /></p>

<h2 id="application">Application</h2>

<p>This section demonstrates how to optimize the hyperparameters of an <code class="language-plaintext highlighter-rouge">XGBRegressor</code> with GPyOpt and how Bayesian optimization performance compares to random search. <code class="language-plaintext highlighter-rouge">XGBRegressor</code> is part of <a href="https://xgboost.readthedocs.io/">XGBoost</a>, a flexible and scalable gradient boosting library. <code class="language-plaintext highlighter-rouge">XGBRegressor</code> implements the scikit-learn estimator API and can be applied to regression problems. Regression is performed on a small <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes">toy dataset</a> that is part of scikit-learn.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span><span class="p">,</span> <span class="n">cross_val_score</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>

<span class="c1"># Load the diabetes dataset (for regression)
</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Instantiate an XGBRegressor with default hyperparameter settings
</span><span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">()</span>

<span class="c1"># and compute a baseline to beat with hyperparameter optimization
</span><span class="n">baseline</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="hyperparameter-tuning-with-random-search">Hyperparameter tuning with random search</h3>

<p>For hyperparameter tuning with random search, we use <code class="language-plaintext highlighter-rouge">RandomSearchCV</code> of scikit-learn and compute a cross-validation score for each randomly selected point in hyperparameter space. Results will be discussed below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hyperparameters to tune and their ranges
</span><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s">"learning_rate"</span><span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
              <span class="s">"gamma"</span><span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
              <span class="s">"max_depth"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span>
              <span class="s">"n_estimators"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">300</span><span class="p">),</span>
              <span class="s">"min_child_weight"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)}</span>

<span class="n">rs</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_dist</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="c1"># Run random search for 25 iterations
</span><span class="n">rs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">);</span>
</code></pre></div></div>

<h3 id="hyperparameter-tuning-with-bayesian-optimization">Hyperparameter tuning with Bayesian optimization</h3>

<p>To tune hyperparameters with Bayesian optimization we implement an objective function <code class="language-plaintext highlighter-rouge">cv_score</code> that takes hyperparameters as input and returns a cross-validation score. Here, we assume that cross-validation at a given point in hyperparameter space is deterministic and therefore set the <code class="language-plaintext highlighter-rouge">exact_feval</code> parameter of <code class="language-plaintext highlighter-rouge">BayesianOptimization</code> to <code class="language-plaintext highlighter-rouge">True</code>. Depending on model fitting and cross-validation details this might not be the case but we ignore that here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bds</span> <span class="o">=</span> <span class="p">[{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'learning_rate'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'gamma'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'max_depth'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">)},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'n_estimators'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'min_child_weight'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)}]</span>

<span class="c1"># Optimization objective
</span><span class="k">def</span> <span class="nf">cv_score</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span>
                <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                              <span class="n">gamma</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                              <span class="n">max_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                              <span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span>
                              <span class="n">min_child_weight</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">cv_score</span><span class="p">,</span>
                                 <span class="n">domain</span><span class="o">=</span><span class="n">bds</span><span class="p">,</span>
                                 <span class="n">model_type</span><span class="o">=</span><span class="s">'GP'</span><span class="p">,</span>
                                 <span class="n">acquisition_type</span> <span class="o">=</span><span class="s">'EI'</span><span class="p">,</span>
                                 <span class="n">acquisition_jitter</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
                                 <span class="n">exact_feval</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">maximize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Only 20 iterations because we have 5 initial random points
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">run_optimization</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="results">Results</h3>

<p>On average, Bayesian optimization finds a better optimium in a smaller number of steps than random search and beats the baseline in almost every run. This trend becomes even more prominent in higher-dimensional search spaces. Here, the search space is 5-dimensional which is rather low to substantially profit from Bayesian optimization. One advantage of random search is that it is trivial to parallelize. Parallelization of Bayesian optimization is much harder and subject to research (see [4], for example).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_rs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">rs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">])</span>
<span class="n">y_bo</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">.</span><span class="n">accumulate</span><span class="p">(</span><span class="o">-</span><span class="n">optimizer</span><span class="p">.</span><span class="n">Y</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Baseline neg. MSE = </span><span class="si">{</span><span class="n">baseline</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Random search neg. MSE = </span><span class="si">{</span><span class="n">y_rs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Bayesian optimization neg. MSE = </span><span class="si">{</span><span class="n">y_bo</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_rs</span><span class="p">,</span> <span class="s">'ro-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Random search'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_bo</span><span class="p">,</span> <span class="s">'bo-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Bayesian optimization'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Neg. MSE'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5000</span><span class="p">,</span> <span class="o">-</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Value of the best sampled CV score'</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Baseline neg. MSE = -3498.95
Random search neg. MSE = -3678.77
Bayesian optimization neg. MSE = -3185.50
</code></pre></div></div>

<p><img src="/blog/img/2018-03-21/output_26_1.png" alt="png" /></p>

<h2 id="references">References</h2>
<ul>
  <li>Kevin P. Murphy. <a href="https://mitpress.mit.edu/books/machine-learning-0">Machine Learning, A Probabilistic Perspective</a>, Chapters 4, 14 and 15.</li>
  <li>Christopher M. Bishop. <a href="http://www.springer.com/de/book/9780387310732">Pattern Recognition and Machine Learning</a>, Chapter 6.</li>
  <li>Carl Edward Rasmussen and Christopher K. I. Williams. <a href="http://www.gaussianprocess.org/gpml/">Gaussian Processes for Machine Learning</a>.</li>
  <li><a href="https://mitpress.mit.edu/books/algorithms-optimization">Algorithms for optimization</a> book.</li>
  <li><a href="https://pyro.ai/examples/bo.html">Pyro</a>’s documentation.</li>
</ul>

<hr />

</article>

<br>
<center><p>Thank you for reading this article 😊</p></center>
<center><p>I'd love to hear your thoughts & feedback in the comment section below!</p></center>
<div class="sharethis-inline-reaction-buttons"></div>
<style>
  #st-1 .st-btn > svg
  {
    width:28px;
  }
</style>
<hr></hr>
<center><p>If you liked it, please share it with your friends and help this blog to grow!</p></center>
<!-- <div class="sharethis-inline-share-buttons"></div> -->
<center><div class="addthis_inline_share_toolbox"></div></center>
<br>
<br>


    
        <script src="https://utteranc.es/client.js"
                repo="Akash-Sharma-1/Utteranc-Comments-Repo-Blog"
                issue-term="pathname"
                theme="github-light"
                crossorigin="anonymous"
                async>
        </script>
    


<script>
    // Add anchors to headers.
    anchors.add();

    // Show images in fancybox.
    $("p img").each(function() {
        $(this).wrapAll('<a data-fancybox="images" data-caption="' + this.alt + '" href="' + this.src + '"></a>');
    });
    $('[data-fancybox="images"]').fancybox({
        transitionEffect: "slide",

        // Support for retina displays.
        afterLoad : function(instance, current) {
            var pixelRatio = window.devicePixelRatio || 1;

            if ( pixelRatio > 1.5 ) {
                current.width  = current.width  / pixelRatio;
                current.height = current.height / pixelRatio;
            }
        }
    });
</script>


<script>
    // Show sidebar on top in mobile devices.
    $(".content").addClass("order-last order-md-first");
    $(".sidebar").addClass("order-first order-md-last");
</script>

</div>
<div class="sidebar col-md-4 mt-2 mt-md-0">
    
    <!-- https://github.com/christian-fei/Simple-Jekyll-Search -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.7.2/simple-jekyll-search.min.js" integrity="sha256-DsgE6Y6cI5eAZoTg8tJ5VBxFs+rnL8smPwt/u8tyAYo=" crossorigin="anonymous"></script>

<div id="search-box" class="mb-2">
    <input class="form-control" type="text" placeholder="Search" aria-label="Search">
    <ul class="mb-0"></ul>
</div>

<script type="text/javascript">
    var search_box = document.getElementById("search-box");
    SimpleJekyllSearch({
        searchInput: search_box.getElementsByTagName("input")[0],
        resultsContainer: search_box.getElementsByTagName("ul")[0],
        json: '/blog/assets/search.json',
        searchResultTemplate: '<li><a href="{url}">{title}</a></li>',
        noResultsText: 'No results found',
        fuzzy: false
    });
</script>

    <!-- https://github.com/tscanlin/tocbot/ -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.7.0/tocbot.css" integrity="sha256-XUbSD3m+xLW27W/mp9kMn+fI9AU2MoBkiMMSVVYWI+o=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.7.0/tocbot.min.js" integrity="sha256-5K/04vUxP94HmQ8f18TgufdyqE69WDJRMNga0JLhAtE=" crossorigin="anonymous"></script>

<div class="toc-container mb-2">
    <p class="name">Table of Contents</p>
    <div class="toc py-2 pr-2 mb-2"></div>
</div>

<script>
    // Show TOC if the article contains any heading.
    if ($("article h1, article  h2, article h3, article h4, article h5, article h6").length == 0) {
        $(".toc-container").addClass("d-none");
    }

    tocbot.init({
        // Where to render the table of contents.
        tocSelector: ".toc",
        // Where to grab the headings to build the table of contents.
        contentSelector: "article",
        // Which headings to grab inside of the contentSelector element.
        headingSelector: "h1, h2, h3, h4, h5, h6",
        collapseDepth: 6,
    });
</script>






</div>

    </div>
</div>

        <footer>
    <div class="container">
        <hr class="my-0">
        <div class="row align-items-center text-muted my-3">
            <div class="col">
                <span>© 2022 Akash Sharma</span>
                <a href="/blog/feed.xml"><span class="feed iconify" data-icon="foundation:rss"></span></a>
            </div>
            <div class="col-auto text-center px-0">
                    <a href="https://akash-sharma-1.github.io/"><span class="github text-center text-muted iconify" data-icon="foundation:web"></span></a>
                    <a href="https://github.com/Akash-Sharma-1"><span class="github text-center text-muted iconify" data-icon="foundation:social-github"></span></a>
                    <a href="https://twitter.com/AkashTheGreat_1"><span class="github text-center text-muted iconify" data-icon="foundation:social-twitter"></span></a>
                    <a href="https://www.linkedin.com/in/akash-sharma-246b67165"><span class="github text-center text-muted iconify" data-icon="foundation:social-linkedin"></span></a>
                    <a href="mailto:akashthegreatlegend@gmail.com"><span class="github text-center text-muted iconify" data-icon="foundation:mail"></span></a>
            </div>
            <div class="col text-right">
                <a href="javascript:$('html,body').animate({ scrollTop: 0 }, 'slow');">TOP</a>
            </div>

        </div>
    </div>
</footer>

    </body>
</html>
