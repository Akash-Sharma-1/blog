<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Exploration of Bayesian Optimization and Gaussian Processes | Akash Sharma’s Blog</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Exploration of Bayesian Optimization and Gaussian Processes" />
<meta name="author" content="Akash Sharma" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset." />
<meta property="og:description" content="Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset." />
<link rel="canonical" href="http://localhost:4000/blog/tech/2022/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html" />
<meta property="og:url" content="http://localhost:4000/blog/tech/2022/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html" />
<meta property="og:site_name" content="Akash Sharma’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-10-14T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Exploration of Bayesian Optimization and Gaussian Processes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Akash Sharma"},"dateModified":"2022-10-14T00:00:00+05:30","datePublished":"2022-10-14T00:00:00+05:30","description":"Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset.","headline":"Exploration of Bayesian Optimization and Gaussian Processes","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/tech/2022/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html"},"url":"http://localhost:4000/blog/tech/2022/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html"}</script>
<!-- End Jekyll SEO tag -->

    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Akash Sharma&apos;s Blog" />
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-W1GFNLSVY2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-W1GFNLSVY2');
</script>

    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
    <link rel="stylesheet" href="/blog/assets/css/main.css">
    <link rel="manifest" href="/blog/manifest.json">

    <!-- Support for Bootstrap https://getbootstrap.com -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js" integrity="sha256-BJeo0qm959uMBGb65z40ejJYGSgR7REI4+CW1fNKwOg=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.15.0/umd/popper.min.js" integrity="sha256-fTuUgtT7O2rqoImwjrhDgbXTKUwyxxujIMRIK7TbuNU=" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

    <!-- Support for pattern background https://github.com/btmills/geopattern -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js" integrity="sha256-AOyAcXCKLfI+FRdiZr4VTj9h9Wwv0YXXa2CPfUWbBS8=" crossorigin="anonymous"></script>

    <!-- Support for SVG icon https://iconify.design -->
    <script src="https://code.iconify.design/1/1.0.0/iconify.min.js"></script>

    <!-- Support for share and reaction btns -->
    <!-- <script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=5fae8b9ee3f5df0012a019ca&product=sop' async='async'></script> -->
    <!-- <script type='text/javascript' src='https://platform-api.sharethis.com/js/sharethis.js#property=650fe938f702ac001998f3a4&product=sop' async='async'></script> -->
    <script src="https://static.elfsight.com/platform/platform.js" data-use-service-core defer></script>

</head>

    <body>
        <header>
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <nav class="container navbar navbar-expand-sm py-2">
        <a class="navbar-brand title p-0" href="/blog/">
          <!-- GITHUB ICON -->
          <!-- <span class="iconify" data-icon="octicon:mark-github"></span> -->
          <!-- Personal Icon -->
          <span class="material-icons" style="vertical-align:middle;font-size:inherit">font_download</span>
            <span style="vertical-align:middle">Akash Sharma's Blog</span>
        </a>

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <div class="navbar-nav ml-auto">
                
                  
                    <a class="nav-item nav-link text-right" href="/blog/">Home</a>
                  
                  

                
                  
                    <a class="nav-item nav-link text-right" href="/blog/classification/">Classification</a>
                  
                  

                
                  
                    <a class="nav-item nav-link text-right" href="/blog/algorithms/">Algorithms</a>
                  
                  

                
                  
                    <a class="nav-item nav-link text-right" href="/blog/about/">About Me</a>
                  
                  

                
                  
                  
                    <a class="nav-item nav-link text-right" href="https://akash-sharma-1.github.io/">Portfolio</a>
                  

                
            </div>
        </div>
    </nav>
</header>

<script>
    
        $(".navbar").addClass("navbar-light");
        $("a[href$='/blog/tech/2022/10/14/Exploration-of-Bayesian-Optimization-&-Gaussian-Processes.html']").addClass("active");
    
</script>

        <div class="titlebar text-light">
    <div class="container">
        
            <!-- Post article title -->
            <div class="title pt-3">Exploration of Bayesian Optimization and Gaussian Processes</div>
            <div class="meta pb-3">
                <span class="pr-3">
                    
                        <span class="iconify" data-icon="octicon:calendar"></span>
                        <span>2022/10/14</span>
                    
                </span>

                
                    <span class="titlebar-info pr-3">
                        <span class="iconify" data-icon="octicon:tag"></span>
                        <a class="text-light" href="/blog/classification/#Machine Learning">Machine Learning</a>
                    </span>
                
                    <span class="titlebar-info pr-3">
                        <span class="iconify" data-icon="octicon:tag"></span>
                        <a class="text-light" href="/blog/classification/#Optimization">Optimization</a>
                    </span>
                
                    <span class="titlebar-info pr-3">
                        <span class="iconify" data-icon="octicon:tag"></span>
                        <a class="text-light" href="/blog/classification/#Probability">Probability</a>
                    </span>
                
                    <span class="titlebar-info pr-3">
                        <span class="iconify" data-icon="octicon:tag"></span>
                        <a class="text-light" href="/blog/classification/#Statistics">Statistics</a>
                    </span>
                

                
                    
                

            </div>
        
    </div>
</div>

<script>
    
        $(".titlebar").geopattern("Exploration of Bayesian Optimization and Gaussian Processes");
    
</script>

<div class="container">
    <div class="row my-2 my-md-4">
        
<div class="content col-md-9">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/3.0.1/github-markdown.min.css" integrity="sha256-HbgiGHMLxHZ3kkAiixyvnaaZFNjNWLYKD/QG6PWaQPc=" crossorigin="anonymous" />

<!-- Support for deep anchor links https://github.com/bryanbraun/anchorjs -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.js" integrity="sha256-0X7DxIkZMaHhAon0xCc/C/YhG6y0dg8Uj8c50+gbu8c=" crossorigin="anonymous"></script>

<!-- Support for fancybox https://github.com/fancyapps/fancybox -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

<!-- Support for MathJax https://github.com/mathjax/mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous" async></script>

<article class="markdown-body">
    <blockquote>
  <p>Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset.</p>
</blockquote>

<h2 id="introduction">Introduction</h2>

<p>Many optimization problems in machine learning are black box optimization problems where the objective function \(f(\mathbf{x})\) is a black box function<sup>[1][2]</sup>. We do not have an analytical expression for \(f\) nor do we know its derivatives. Evaluation of the function is restricted to sampling at a point \(\mathbf{x}\) and getting a possibly noisy response.</p>

<p>If \(f\) is cheap to evaluate we could sample at many points e.g. via grid search, random search or numeric gradient estimation. However, if function evaluation is expensive e.g. tuning hyperparameters of  a deep neural network, probe drilling for oil at given geographic coordinates or evaluating the effectiveness of a drug candidate taken from a chemical search space then it is important to minimize the number of samples drawn from the black box function \(f\).</p>

<p>This is the domain where Bayesian optimization techniques are most useful. They attempt to find the global optimimum in a minimum number of steps. Bayesian optimization incorporates prior belief about \(f\) and updates the prior with samples drawn from \(f\) to get a posterior that better approximates \(f\). The model used for approximating the objective function is called <em>surrogate model</em>. Bayesian optimization also uses an <em>acquisition function</em> that directs sampling to areas where an improvement over the current best observation is likely.</p>

<h3 id="surrogate-model">Surrogate model</h3>

<p>A popular surrogate model for Bayesian optimization are <a href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian processes</a> (GPs). I will be also writing about Gaussian processes in a separate post. If you are not familiar with GPs I recommend reading it first. GPs define a prior over functions and we can use them to incorporate prior beliefs about the objective function (smoothness, …). The GP posterior is cheap to evaluate and is used to propose points in the search space where sampling is likely to yield an improvement.</p>

<h3 id="acquisition-functions">Acquisition functions</h3>

<p>Proposing sampling points in the search space is done by acquisition functions. They trade off exploitation and exploration. Exploitation means sampling where the surrogate model predicts a high objective and exploration means sampling at locations where the prediction uncertainty is high. Both correspond to high acquisition function values and the goal is to maximize the acquisition function to determine the next sampling point.</p>

<p>More formally, the objective function \(f\) will be sampled at \(\mathbf{x}\_t = \operatorname{argmax}\_{\mathbf{x}} u(\mathbf{x} \lvert \mathcal{D}\_{1:t-1})\) where \(u\) is the acquisition function and \(\mathcal{D}\_{1:t-1} = \{(\mathbf{x}\_1, y_1),...,(\mathbf{x}\_{t-1}, y\_{t-1})\}\) are the \(t-1\) samples drawn from \(f\) so far. Popular acquisition functions are <em>maximum probability of improvement</em> (MPI), <em>expected improvement</em> (EI) and <em>upper confidence bound</em> (UCB)<sup>[1]</sup>. In the following, we will use the expected improvement (EI) which is most widely used and described further below.</p>

<h3 id="optimization-algorithm">Optimization algorithm</h3>

<p>The Bayesian optimization procedure is as follows. For \(t = 1,2,...\) repeat:</p>

<ul>
  <li>Find the next sampling point \(\mathbf{x}\_{t}\) by optimizing the acquisition function over the GP: \(\mathbf{x}\_t = \operatorname{argmax}\_{\mathbf{x}} u(\mathbf{x} \lvert \mathcal{D}\_{1:t-1})\)</li>
  <li>Obtain a possibly noisy sample \(y_t = f(\mathbf{x}_t) + \epsilon_t\) from the objective function \(f\).</li>
  <li>Add the sample to previous samples \(\mathcal{D}\_{1:t} = \{\mathcal{D}\_{1:t-1}, (\mathbf{x}\_t,y\_t)\}\) and update the GP.</li>
</ul>

<h3 id="expected-improvement">Expected improvement</h3>

<p>Expected improvement is defined as</p>

\[\operatorname{EI}(\mathbf{x}) = \mathbb{E}\max(f(\mathbf{x}) - f(\mathbf{x}^+), 0)\tag{1}\]

<p>where \(f(\mathbf{x}^+)\) is the value of the best sample so far and \(\mathbf{x}^+\) is the location of that sample i.e. \(\mathbf{x}^+ = \operatorname{argmax}\_{\mathbf{x}\_i \in \mathbf{x}\_{1:t}} f(\mathbf{x}\_i)\). The expected improvement can be evaluated analytically under the GP model<sup>[3]</sup>:</p>

\[\operatorname{EI}(\mathbf{x}) =
\begin{cases}
(\mu(\mathbf{x}) - f(\mathbf{x}^+) - \xi)\Phi(Z) + \sigma(\mathbf{x})\phi(Z)  &amp;\text{if}\ \sigma(\mathbf{x}) &gt; 0 \\
0 &amp; \text{if}\ \sigma(\mathbf{x}) = 0
\end{cases}\tag{2}\]

<p>where</p>

\[Z =
\begin{cases}
\frac{\mu(\mathbf{x}) - f(\mathbf{x}^+) - \xi}{\sigma(\mathbf{x})} &amp;\text{if}\ \sigma(\mathbf{x}) &gt; 0 \\
0 &amp; \text{if}\ \sigma(\mathbf{x}) = 0
\end{cases}\]

<p>where \(\mu(\mathbf{x})\) and \(\sigma(\mathbf{x})\) are the mean and the standard deviation of the GP posterior predictive at \(\mathbf{x}\), respectively. \(\Phi\) and \(\phi\) are the CDF and PDF of the standard normal distribution, respectively. The first summation term in Equation (2) is the exploitation term and second summation term is the exploration term.</p>

<p>Parameter \(\xi\) in Equation (2) determines the amount of exploration during optimization and higher \(\xi\) values lead to more exploration. In other words, with increasing \(\xi\) values, the importance of improvements predicted by the GP posterior mean \(\mu(\mathbf{x})\) decreases relative to the importance of potential improvements in regions of high prediction uncertainty, represented by large \(\sigma(\mathbf{x})\) values. A recommended default value for \(\xi\) is \(0.01\).</p>

<p>With this minimum of theory we can start implementing Bayesian optimization. The next section shows a basic implementation with plain NumPy and SciPy, later sections demonstrate how to use existing libraries. Finally, Bayesian optimization is used to tune the hyperparameters of a tree-based regression model.</p>

<h2 id="implementation-with-numpy-and-scipy">Implementation with NumPy and SciPy</h2>

<p>In this section, we will implement the acquisition function and its optimization in plain NumPy and SciPy and use scikit-learn for the Gaussian process implementation. Although we have an analytical expression of the optimization objective <code class="language-plaintext highlighter-rouge">f</code> in the following example, we treat is as black box and iteratively approximate it with a Gaussian process during Bayesian optimization. Furthermore, samples drawn from the objective function are noisy and the noise level is given by the <code class="language-plaintext highlighter-rouge">noise</code> variable. Optimization is done within given <code class="language-plaintext highlighter-rouge">bounds</code>. We also assume that there exist two initial samples in <code class="language-plaintext highlighter-rouge">X_init</code> and <code class="language-plaintext highlighter-rouge">Y_init</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">bounds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.7</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">X_init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">]])</span>
<span class="n">Y_init</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X_init</span><span class="p">)</span>
</code></pre></div></div>

<p>The following plot shows the noise-free objective function, the amount of noise by plotting a large number of samples and the two initial samples.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Dense grid of points within bounds
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="mf">0.01</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Noise-free objective function values at X
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plot optimization objective with noise level
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="s">'y--'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Noise-free objective'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s">'bx'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Noisy samples'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_init</span><span class="p">,</span> <span class="n">Y_init</span><span class="p">,</span> <span class="s">'kx'</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Initial samples'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="/blog/assets/images/bayesian-optimization/output_4_0.png" alt="png" /></p>

<p>Goal is to find the global optimum on the left in a small number of steps. The next step is to implement the acquisition function defined in Equation (2) as <code class="language-plaintext highlighter-rouge">expected_improvement</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">expected_improvement</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="s">'''
    Computes the EI at points X based on existing samples X_sample
    and Y_sample using a Gaussian process surrogate model.

    Args:
        X: Points at which EI shall be computed (m x d).
        X_sample: Sample locations (n x d).
        Y_sample: Sample values (n x 1).
        gpr: A GaussianProcessRegressor fitted to samples.
        xi: Exploitation-exploration trade-off parameter.

    Returns:
        Expected improvements at points X.
    '''</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">gpr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">mu_sample</span> <span class="o">=</span> <span class="n">gpr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>

    <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Needed for noise-based model,
</span>    <span class="c1"># otherwise use np.max(Y_sample).
</span>    <span class="c1"># See also section 2.4 in [...]
</span>    <span class="n">mu_sample_opt</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">mu_sample</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">np</span><span class="p">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s">'warn'</span><span class="p">):</span>
        <span class="n">imp</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">-</span> <span class="n">mu_sample_opt</span> <span class="o">-</span> <span class="n">xi</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">imp</span> <span class="o">/</span> <span class="n">sigma</span>
        <span class="n">ei</span> <span class="o">=</span> <span class="n">imp</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        <span class="n">ei</span><span class="p">[</span><span class="n">sigma</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">return</span> <span class="n">ei</span>
</code></pre></div></div>

<p>We also need a function that proposes the next sampling point by computing the location of the acquisition function maximum. Optimization is restarted <code class="language-plaintext highlighter-rouge">n_restarts</code> times to avoid local optima.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="k">def</span> <span class="nf">propose_location</span><span class="p">(</span><span class="n">acquisition</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">bounds</span><span class="p">,</span> <span class="n">n_restarts</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
    <span class="s">'''
    Proposes the next sampling point by optimizing the acquisition function.

    Args:
        acquisition: Acquisition function.
        X_sample: Sample locations (n x d).
        Y_sample: Sample values (n x 1).
        gpr: A GaussianProcessRegressor fitted to samples.

    Returns:
        Location of the acquisition function maximum.
    '''</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">X_sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">min_x</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">min_obj</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="c1"># Minimization objective is the negative acquisition function
</span>        <span class="k">return</span> <span class="o">-</span><span class="n">acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">)</span>

    <span class="c1"># Find the best optimum by starting from n_restart different random points.
</span>    <span class="k">for</span> <span class="n">x0</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_restarts</span><span class="p">,</span> <span class="n">dim</span><span class="p">)):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">min_obj</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'L-BFGS-B'</span><span class="p">)</span>        
        <span class="k">if</span> <span class="n">res</span><span class="p">.</span><span class="n">fun</span> <span class="o">&lt;</span> <span class="n">min_val</span><span class="p">:</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">fun</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">min_x</span> <span class="o">=</span> <span class="n">res</span><span class="p">.</span><span class="n">x</span>           

    <span class="k">return</span> <span class="n">min_x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we have all components needed to run Bayesian optimization with the <a href="#Optimization-algorithm">algorithm</a> outlined above. The Gaussian process in the following example is configured with a <a href="http://scikit-learn.org/stable/modules/gaussian_process.html#matern-kernel">Matérn kernel</a> which is a generalization of the squared exponential kernel or RBF kernel. The known noise level is configured with the <code class="language-plaintext highlighter-rouge">alpha</code> parameter.</p>

<p>Bayesian optimization runs for 10 iterations. In each iteration, a row with two plots is produced. The left plot shows the noise-free objective function, the surrogate function which is the GP posterior predictive mean, the 95% confidence interval of the mean and the noisy samples obtained from the objective function so far. The right plot shows the acquisition function. The vertical dashed line in both plots shows the proposed sampling point for the next iteration which corresponds to the maximum of the acquisition function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ConstantKernel</span><span class="p">,</span> <span class="n">Matern</span>
<span class="kn">from</span> <span class="nn">bayesian_optimization_util</span> <span class="kn">import</span> <span class="n">plot_approximation</span><span class="p">,</span> <span class="n">plot_acquisition</span>

<span class="c1"># Gaussian process with Mat??rn kernel as surrogate model
</span><span class="n">m52</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">Matern</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">gpr</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Initialize samples
</span><span class="n">X_sample</span> <span class="o">=</span> <span class="n">X_init</span>
<span class="n">Y_sample</span> <span class="o">=</span> <span class="n">Y_init</span>

<span class="c1"># Number of iterations
</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="c1"># Update Gaussian process with existing samples
</span>    <span class="n">gpr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">)</span>

    <span class="c1"># Obtain next sampling point from the acquisition function (expected_improvement)
</span>    <span class="n">X_next</span> <span class="o">=</span> <span class="n">propose_location</span><span class="p">(</span><span class="n">expected_improvement</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">bounds</span><span class="p">)</span>

    <span class="c1"># Obtain next noisy sample from the objective function
</span>    <span class="n">Y_next</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X_next</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

    <span class="c1"># Plot samples, surrogate function, noise-free objective and next sampling location
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plot_approximation</span><span class="p">(</span><span class="n">gpr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Iteration </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_iter</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plot_acquisition</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">expected_improvement</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">,</span> <span class="n">gpr</span><span class="p">),</span> <span class="n">X_next</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="n">i</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Add sample to previous samples
</span>    <span class="n">X_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">X_next</span><span class="p">))</span>
    <span class="n">Y_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">Y_sample</span><span class="p">,</span> <span class="n">Y_next</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/blog/assets/images/bayesian-optimization/output_10_0.png" alt="png" /></p>

<p>Note how the two initial samples initially drive search into the direction of the local maximum on the right side but exploration allows the algorithm to escape from that local optimum and find the global optimum on the left side. Also note how sampling point proposals often fall within regions of high uncertainty (exploration) and are not only driven by the highest surrogate function values (exploitation).</p>

<p>A convergence plot reveals how many iterations are needed the find a maximum and if the sampling point proposals stay around that maximum i.e. converge to small proposal differences between consecutive steps.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">bayesian_optimization_util</span> <span class="kn">import</span> <span class="n">plot_convergence</span>

<span class="n">plot_convergence</span><span class="p">(</span><span class="n">X_sample</span><span class="p">,</span> <span class="n">Y_sample</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/images/bayesian-optimization/output_12_0.png" alt="png" /></p>

<h2 id="bayesian-optimization-libraries">Bayesian optimization libraries</h2>

<p>There are numerous Bayesian optimization libraries out there and giving a comprehensive overview is not the goal of this article. Instead, I’ll pick two that I used in the past and show the minimum setup needed to get the previous example running.</p>

<h3 id="scikit-optimize">Scikit-optimize</h3>

<p><a href="https://scikit-optimize.github.io/">Scikit-optimize</a> is a library for sequential model-based optimization that is based on <a href="http://scikit-learn.org/">scikit-learn</a>. It also supports Bayesian optimization using Gaussian processes. The API is designed around minimization, hence, we have to provide negative objective function values.  The results obtained here slightly differ from previous results because of non-deterministic optimization behavior and different noisy samples drawn from the objective function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">skopt</span> <span class="kn">import</span> <span class="n">gp_minimize</span>
<span class="kn">from</span> <span class="nn">skopt.learning</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="nn">skopt.learning.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ConstantKernel</span><span class="p">,</span> <span class="n">Matern</span>

<span class="c1"># Use custom kernel and estimator to match previous example
</span><span class="n">m52</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">Matern</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">gpr</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">m52</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">gp_minimize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">))[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">bounds</span><span class="p">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">base_estimator</span><span class="o">=</span><span class="n">gpr</span><span class="p">,</span>
                <span class="n">acq_func</span><span class="o">=</span><span class="s">'EI'</span><span class="p">,</span>      <span class="c1"># expected improvement
</span>                <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>            <span class="c1"># exploitation-exploration trade-off
</span>                <span class="n">n_calls</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>         <span class="c1"># number of iterations
</span>                <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># initial samples are provided
</span>                <span class="n">x0</span><span class="o">=</span><span class="n">X_init</span><span class="p">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="c1"># initial samples
</span>                <span class="n">y0</span><span class="o">=-</span><span class="n">Y_init</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>

<span class="c1"># Fit GP model to samples for plotting results
</span><span class="n">gpr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">x_iters</span><span class="p">,</span> <span class="o">-</span><span class="n">r</span><span class="p">.</span><span class="n">func_vals</span><span class="p">)</span>

<span class="c1"># Plot the fitted model and the noisy samples
</span><span class="n">plot_approximation</span><span class="p">(</span><span class="n">gpr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">r</span><span class="p">.</span><span class="n">x_iters</span><span class="p">,</span> <span class="o">-</span><span class="n">r</span><span class="p">.</span><span class="n">func_vals</span><span class="p">,</span> <span class="n">show_legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/images/bayesian-optimization/output_14_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_convergence</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">x_iters</span><span class="p">),</span> <span class="o">-</span><span class="n">r</span><span class="p">.</span><span class="n">func_vals</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/blog/assets/images/bayesian-optimization/output_15_0.png" alt="png" /></p>

<h2 id="gpyopt">GPyOpt</h2>

<p><a href="http://sheffieldml.github.io/GPyOpt/">GPyOpt</a> is a Bayesian optimization library based on <a href="https://sheffieldml.github.io/GPy/">GPy</a>. The abstraction level of the API is comparable to that of scikit-optimize. The <code class="language-plaintext highlighter-rouge">BayesianOptimization</code> API provides a <code class="language-plaintext highlighter-rouge">maximize</code> parameter to configure whether the objective function shall be maximized or minimized (default). In version 1.2.1, this seems to be ignored when providing initial samples, so we have to negate their target values manually in the following example. Also, the built-in <code class="language-plaintext highlighter-rouge">plot_acquisition</code> and <code class="language-plaintext highlighter-rouge">plot_convergence</code> methods display the minimization result in any case. Again, the results obtained here slightly differ from previous results because of non-deterministic optimization behavior and different noisy samples drawn from the objective function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">GPy</span>
<span class="kn">import</span> <span class="nn">GPyOpt</span>

<span class="kn">from</span> <span class="nn">GPyOpt.methods</span> <span class="kn">import</span> <span class="n">BayesianOptimization</span>

<span class="n">kernel</span> <span class="o">=</span> <span class="n">GPy</span><span class="p">.</span><span class="n">kern</span><span class="p">.</span><span class="n">Matern52</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">variance</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lengthscale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">bds</span> <span class="o">=</span> <span class="p">[{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'X'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="n">bounds</span><span class="p">.</span><span class="n">ravel</span><span class="p">()}]</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">f</span><span class="p">,</span>
                                 <span class="n">domain</span><span class="o">=</span><span class="n">bds</span><span class="p">,</span>
                                 <span class="n">model_type</span><span class="o">=</span><span class="s">'GP'</span><span class="p">,</span>
                                 <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span>
                                 <span class="n">acquisition_type</span> <span class="o">=</span><span class="s">'EI'</span><span class="p">,</span>
                                 <span class="n">acquisition_jitter</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
                                 <span class="n">X</span><span class="o">=</span><span class="n">X_init</span><span class="p">,</span>
                                 <span class="n">Y</span><span class="o">=-</span><span class="n">Y_init</span><span class="p">,</span>
                                 <span class="n">noise_var</span> <span class="o">=</span> <span class="n">noise</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                                 <span class="n">exact_feval</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                 <span class="n">normalize_Y</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                 <span class="n">maximize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">optimizer</span><span class="p">.</span><span class="n">run_optimization</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">plot_acquisition</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/assets/images/bayesian-optimization/output_17_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="p">.</span><span class="n">plot_convergence</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/blog/assets/images/bayesian-optimization/output_18_0.png" alt="png" /></p>

<h2 id="application">Application</h2>

<p>This section demonstrates how to optimize the hyperparameters of an <code class="language-plaintext highlighter-rouge">XGBRegressor</code> with GPyOpt and how Bayesian optimization performance compares to random search. <code class="language-plaintext highlighter-rouge">XGBRegressor</code> is part of <a href="https://xgboost.readthedocs.io/">XGBoost</a>, a flexible and scalable gradient boosting library. <code class="language-plaintext highlighter-rouge">XGBRegressor</code> implements the scikit-learn estimator API and can be applied to regression problems. Regression is performed on a small <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes">toy dataset</a> that is part of scikit-learn.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span><span class="p">,</span> <span class="n">cross_val_score</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>

<span class="c1"># Load the diabetes dataset (for regression)
</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Instantiate an XGBRegressor with default hyperparameter settings
</span><span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">()</span>

<span class="c1"># and compute a baseline to beat with hyperparameter optimization
</span><span class="n">baseline</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="hyperparameter-tuning-with-random-search">Hyperparameter tuning with random search</h3>

<p>For hyperparameter tuning with random search, we use <code class="language-plaintext highlighter-rouge">RandomSearchCV</code> of scikit-learn and compute a cross-validation score for each randomly selected point in hyperparameter space. Results will be discussed below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Hyperparameters to tune and their ranges
</span><span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span><span class="s">"learning_rate"</span><span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
              <span class="s">"gamma"</span><span class="p">:</span> <span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
              <span class="s">"max_depth"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span>
              <span class="s">"n_estimators"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">300</span><span class="p">),</span>
              <span class="s">"min_child_weight"</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)}</span>

<span class="n">rs</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">xgb</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_dist</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="c1"># Run random search for 25 iterations
</span><span class="n">rs</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">);</span>
</code></pre></div></div>

<h3 id="hyperparameter-tuning-with-bayesian-optimization">Hyperparameter tuning with Bayesian optimization</h3>

<p>To tune hyperparameters with Bayesian optimization we implement an objective function <code class="language-plaintext highlighter-rouge">cv_score</code> that takes hyperparameters as input and returns a cross-validation score. Here, we assume that cross-validation at a given point in hyperparameter space is deterministic and therefore set the <code class="language-plaintext highlighter-rouge">exact_feval</code> parameter of <code class="language-plaintext highlighter-rouge">BayesianOptimization</code> to <code class="language-plaintext highlighter-rouge">True</code>. Depending on model fitting and cross-validation details this might not be the case but we ignore that here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bds</span> <span class="o">=</span> <span class="p">[{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'learning_rate'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'gamma'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'continuous'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'max_depth'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">)},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'n_estimators'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">300</span><span class="p">)},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'min_child_weight'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'discrete'</span><span class="p">,</span> <span class="s">'domain'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)}]</span>

<span class="c1"># Optimization objective
</span><span class="k">def</span> <span class="nf">cv_score</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span>
                <span class="n">XGBRegressor</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                              <span class="n">gamma</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                              <span class="n">max_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                              <span class="n">n_estimators</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span>
                              <span class="n">min_child_weight</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">BayesianOptimization</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">cv_score</span><span class="p">,</span>
                                 <span class="n">domain</span><span class="o">=</span><span class="n">bds</span><span class="p">,</span>
                                 <span class="n">model_type</span><span class="o">=</span><span class="s">'GP'</span><span class="p">,</span>
                                 <span class="n">acquisition_type</span> <span class="o">=</span><span class="s">'EI'</span><span class="p">,</span>
                                 <span class="n">acquisition_jitter</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
                                 <span class="n">exact_feval</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">maximize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Only 20 iterations because we have 5 initial random points
</span><span class="n">optimizer</span><span class="p">.</span><span class="n">run_optimization</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="results">Results</h3>

<p>On average, Bayesian optimization finds a better optimium in a smaller number of steps than random search and beats the baseline in almost every run. This trend becomes even more prominent in higher-dimensional search spaces. Here, the search space is 5-dimensional which is rather low to substantially profit from Bayesian optimization. One advantage of random search is that it is trivial to parallelize. Parallelization of Bayesian optimization is much harder and subject to research (see [4], for example).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_rs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">rs</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s">'mean_test_score'</span><span class="p">])</span>
<span class="n">y_bo</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">.</span><span class="n">accumulate</span><span class="p">(</span><span class="o">-</span><span class="n">optimizer</span><span class="p">.</span><span class="n">Y</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Baseline neg. MSE = </span><span class="si">{</span><span class="n">baseline</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Random search neg. MSE = </span><span class="si">{</span><span class="n">y_rs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Bayesian optimization neg. MSE = </span><span class="si">{</span><span class="n">y_bo</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_rs</span><span class="p">,</span> <span class="s">'ro-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Random search'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_bo</span><span class="p">,</span> <span class="s">'bo-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Bayesian optimization'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Neg. MSE'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5000</span><span class="p">,</span> <span class="o">-</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Value of the best sampled CV score'</span><span class="p">);</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">();</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Baseline neg. MSE = -3498.95
Random search neg. MSE = -3678.77
Bayesian optimization neg. MSE = -3185.50
</code></pre></div></div>

<p><img src="/blog/assets/images/bayesian-optimization/output_26_1.png" alt="png" /></p>

<h2 id="references">References</h2>
<ul>
  <li>Kevin P. Murphy. <a href="https://mitpress.mit.edu/books/machine-learning-0">Machine Learning, A Probabilistic Perspective</a>, Chapters 4, 14 and 15.</li>
  <li>Christopher M. Bishop. <a href="http://www.springer.com/de/book/9780387310732">Pattern Recognition and Machine Learning</a>, Chapter 6.</li>
  <li>Carl Edward Rasmussen and Christopher K. I. Williams. <a href="http://www.gaussianprocess.org/gpml/">Gaussian Processes for Machine Learning</a>.</li>
  <li><a href="https://mitpress.mit.edu/books/algorithms-optimization">Algorithms for optimization</a> book.</li>
  <li><a href="https://pyro.ai/examples/bo.html">Pyro</a>’s documentation.</li>
</ul>

<hr />

</article>

<br>
<center><p>Thank you for reading this article 😊</p></center>
<center><p>I'd love to hear your thoughts & feedback in the comment section below!</p></center>

<div align="center" class="mt-2">
    <svg width="131" height="42" viewBox="0 0 131 42" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M0.5 0.5H116C124.008 0.5 130.5 6.99187 130.5 15V41.5H15C6.99187 41.5 0.5 35.0081 0.5 27V0.5Z" fill="black" stroke="#ACACAC"/>
        <path d="M17.9605 24.1575C21.4266 26.9643 26.3836 26.9643 29.8497 24.1575L28.5095 22.5026C25.8248 24.6766 21.9854 24.6766 19.3007 22.5026L17.9605 24.1575Z" fill="white"/>
        <path d="M19.404 20.5134V17.6365H21.5336V20.5134H19.404Z" fill="white"/>
        <path d="M26.012 17.6365V20.5134H28.1415V17.6365H26.012Z" fill="white"/>
        <path fill-rule="evenodd" clip-rule="evenodd" d="M35 21.5C35 27.8513 29.8513 33 23.5 33C17.1487 33 12 27.8513 12 21.5C12 15.1487 17.1487 10 23.5 10C29.8513 10 35 15.1487 35 21.5ZM32.8705 21.5C32.8705 26.6752 28.6752 30.8705 23.5 30.8705C18.3248 30.8705 14.1295 26.6752 14.1295 21.5C14.1295 16.3248 18.3248 12.1295 23.5 12.1295C28.6752 12.1295 32.8705 16.3248 32.8705 21.5Z" fill="white"/>
        <path d="M48.2896 22.1781C49.2796 22.1781 50.088 22.4414 50.7148 22.9681C51.3474 23.4889 51.6638 24.356 51.6638 25.5692V32.0851H49.098V26.1995C49.098 25.6905 49.0307 25.2999 48.8959 25.0277C48.6499 24.5305 48.1813 24.282 47.49 24.282C46.6407 24.282 46.0578 24.646 45.7415 25.3739C45.5775 25.7586 45.4954 26.2498 45.4954 26.8475V32.0851H43V22.4266H45.4164V23.8381C45.7385 23.341 46.0432 22.9829 46.3302 22.764C46.8457 22.3734 47.4988 22.1781 48.2896 22.1781Z" fill="white"/>
        <path d="M57.5604 30.3008C58.2926 30.3008 58.855 30.0374 59.2475 29.5107C59.6399 28.984 59.8362 28.2353 59.8362 27.2648C59.8362 26.2942 59.6399 25.5485 59.2475 25.0277C58.855 24.501 58.2926 24.2376 57.5604 24.2376C56.8282 24.2376 56.2629 24.501 55.8646 25.0277C55.4721 25.5485 55.2758 26.2942 55.2758 27.2648C55.2758 28.2353 55.4721 28.984 55.8646 29.5107C56.2629 30.0374 56.8282 30.3008 57.5604 30.3008ZM62.4634 27.2648C62.4634 28.6851 62.0592 29.9013 61.2508 30.9133C60.4425 31.9194 59.2152 32.4225 57.5692 32.4225C55.9231 32.4225 54.6959 31.9194 53.8875 30.9133C53.0791 29.9013 52.6749 28.6851 52.6749 27.2648C52.6749 25.8681 53.0791 24.6578 53.8875 23.6339C54.6959 22.6101 55.9231 22.0982 57.5692 22.0982C59.2152 22.0982 60.4425 22.6101 61.2508 23.6339C62.0592 24.6578 62.4634 25.8681 62.4634 27.2648Z" fill="white"/>
        <path d="M62.5608 24.2997V22.4977H63.8964V19.799H66.3742V22.4977H67.9295V24.2997H66.3742V29.4131C66.3742 29.8096 66.424 30.0581 66.5236 30.1587C66.6232 30.2534 66.9278 30.3008 67.4374 30.3008C67.5136 30.3008 67.5927 30.3008 67.6747 30.3008C67.7626 30.2949 67.8475 30.2889 67.9295 30.283V32.1739L66.7433 32.2183C65.56 32.2597 64.7516 32.0526 64.3181 31.5969C64.037 31.3069 63.8964 30.8601 63.8964 30.2564V24.2997H62.5608Z" fill="white"/>
        <path d="M77.602 22.1958C78.8615 22.1958 79.8456 22.6545 80.5544 23.5718C81.2691 24.4891 81.6264 25.6728 81.6264 27.1227C81.6264 28.6259 81.2749 29.8717 80.572 30.8601C79.869 31.8484 78.8878 32.3426 77.6284 32.3426C76.8376 32.3426 76.202 32.1828 75.7217 31.8632C75.4346 31.6738 75.1242 31.3424 74.7903 30.8689V32.0851H72.3388V19.0178H74.8342V23.6695C75.1505 23.2197 75.4991 22.8764 75.8798 22.6397C76.3309 22.3438 76.905 22.1958 77.602 22.1958ZM76.9606 30.2564C77.605 30.2564 78.1058 29.993 78.4631 29.4663C78.8205 28.9396 78.9991 28.2472 78.9991 27.389C78.9991 26.7025 78.9113 26.1344 78.7355 25.6846C78.4016 24.8324 77.7866 24.4063 76.8903 24.4063C75.9823 24.4063 75.3585 24.8235 75.0187 25.658C74.843 26.1018 74.7551 26.6759 74.7551 27.3802C74.7551 28.2087 74.9367 28.8952 75.2999 29.4397C75.6631 29.9842 76.2167 30.2564 76.9606 30.2564Z" fill="white"/>
        <path d="M83.0945 33.9405L83.4108 33.9582C83.6568 33.9701 83.8911 33.9612 84.1137 33.9316C84.3363 33.902 84.5238 33.8339 84.6761 33.7274C84.8225 33.6268 84.9573 33.4167 85.0803 33.0971C85.2092 32.7775 85.2619 32.5822 85.2385 32.5112L81.7237 22.4089H84.5092L86.6004 29.5462L88.5774 22.4089H91.2398L87.9536 31.9253C87.3209 33.76 86.8201 34.8963 86.451 35.3342C86.082 35.7781 85.3439 36 84.2368 36C84.0142 36 83.8355 35.997 83.7008 35.9911C83.566 35.9911 83.3639 35.9822 83.0945 35.9645V33.9405Z" fill="white"/>
        <path d="M97.783 27.1405H101.069L99.4525 21.9916L97.783 27.1405ZM97.95 19H101.008L105.594 32.0851H102.66L101.825 29.3953H97.0537L96.1575 32.0851H93.3281L97.95 19Z" fill="white"/>
        <path d="M110.59 32.0851H107.902V19H110.59V32.0851Z" fill="white"/>
        <path d="M106.306 19H112V21.2258H106.306V19Z" fill="white"/>
        <path d="M106.306 29.8624H112V32.0882H106.306V29.8624Z" fill="white"/>
        <path d="M42.9754 9.89597L43.9953 13.8667L45.0301 9.89597H46.0303L47.07 13.8432L48.1544 9.89597H49.0456L47.5058 14.9347H46.5799L45.5005 11.0345L44.4558 14.9347H43.5299L42 9.89597H42.9754Z" fill="white"/>
        <path d="M49.8156 9.89597H50.6622V10.7663C50.7316 10.597 50.9016 10.3915 51.1722 10.15C51.4429 9.90538 51.7548 9.78306 52.108 9.78306C52.1245 9.78306 52.1526 9.78463 52.1922 9.78777C52.2318 9.7909 52.2994 9.79718 52.3952 9.80659V10.7005C52.3424 10.6911 52.2928 10.6848 52.2466 10.6817C52.2037 10.6785 52.1559 10.6769 52.1031 10.6769C51.6541 10.6769 51.3092 10.815 51.0682 11.091C50.8273 11.3638 50.7068 11.679 50.7068 12.0366V14.9347H49.8156V9.89597Z" fill="white"/>
        <path d="M53.0662 9.9195H53.9722V14.9347H53.0662V9.9195ZM53.0662 8.02352H53.9722V8.98327H53.0662V8.02352Z" fill="white"/>
        <path d="M55.4008 8.48928H56.3019V9.89597H57.1485V10.5876H56.3019V13.8761C56.3019 14.0518 56.3646 14.1694 56.49 14.229C56.5593 14.2635 56.6749 14.2807 56.8366 14.2807C56.8795 14.2807 56.9257 14.2807 56.9752 14.2807C57.0248 14.2776 57.0825 14.2729 57.1485 14.2666V14.9347C57.0462 14.9629 56.9389 14.9833 56.8267 14.9958C56.7178 15.0084 56.599 15.0146 56.4702 15.0146C56.0543 15.0146 55.7721 14.9143 55.6236 14.7135C55.475 14.5097 55.4008 14.2462 55.4008 13.9232V10.5876H54.6828V9.89597H55.4008V8.48928Z" fill="white"/>
        <path d="M58.1215 8.48928H59.0227V9.89597H59.8693V10.5876H59.0227V13.8761C59.0227 14.0518 59.0854 14.1694 59.2108 14.229C59.2801 14.2635 59.3957 14.2807 59.5574 14.2807C59.6003 14.2807 59.6465 14.2807 59.696 14.2807C59.7455 14.2776 59.8033 14.2729 59.8693 14.2666V14.9347C59.767 14.9629 59.6597 14.9833 59.5475 14.9958C59.4386 15.0084 59.3197 15.0146 59.191 15.0146C58.7751 15.0146 58.4929 14.9143 58.3444 14.7135C58.1958 14.5097 58.1215 14.2462 58.1215 13.9232V10.5876H57.4036V9.89597H58.1215V8.48928Z" fill="white"/>
        <path d="M62.8723 9.78306C63.2486 9.78306 63.6134 9.86775 63.9666 10.0371C64.3197 10.2033 64.5888 10.4198 64.7736 10.6864C64.9518 10.9404 65.0707 11.2368 65.1301 11.5755C65.1829 11.8076 65.2093 12.1777 65.2093 12.6858H61.3226C61.3391 13.1971 61.4662 13.6079 61.7039 13.9185C61.9415 14.2258 62.3095 14.3795 62.808 14.3795C63.2734 14.3795 63.6447 14.2337 63.922 13.942C64.0804 13.7726 64.1927 13.5766 64.2587 13.3539H65.135C65.1119 13.5389 65.0344 13.7459 64.9023 13.9749C64.7736 14.2007 64.6284 14.3858 64.4666 14.5301C64.196 14.781 63.8609 14.9503 63.4615 15.0382C63.247 15.0883 63.0044 15.1134 62.7337 15.1134C62.0735 15.1134 61.5141 14.886 61.0552 14.4313C60.5964 13.9733 60.367 13.3335 60.367 12.5118C60.367 11.7026 60.5981 11.0455 61.0602 10.5405C61.5223 10.0355 62.1264 9.78306 62.8723 9.78306ZM64.2933 12.0131C64.257 11.6461 64.1729 11.3528 64.0408 11.1333C63.7966 10.7256 63.3889 10.5217 62.8179 10.5217C62.4086 10.5217 62.0653 10.6628 61.788 10.9451C61.5108 11.2243 61.3639 11.5802 61.3474 12.0131H64.2933Z" fill="white"/>
        <path d="M66.2071 9.89597H67.0537V10.6111C67.3046 10.3163 67.5703 10.1045 67.8509 9.97595C68.1315 9.84736 68.4434 9.78306 68.7867 9.78306C69.5392 9.78306 70.0476 10.0324 70.3116 10.5311C70.4569 10.804 70.5295 11.1945 70.5295 11.7026V14.9347H69.6234V11.759C69.6234 11.4516 69.5756 11.2039 69.4798 11.0157C69.3214 10.702 69.0342 10.5452 68.6183 10.5452C68.4071 10.5452 68.2338 10.5656 68.0984 10.6064C67.8542 10.6754 67.6396 10.8134 67.4548 11.0204C67.3062 11.1866 67.2089 11.3591 67.1627 11.5379C67.1198 11.7135 67.0983 11.966 67.0983 12.2953V14.9347H66.2071V9.89597Z" fill="white"/>
        <path d="M74.4015 8H75.268V10.5076C75.4628 10.2661 75.6955 10.0826 75.9661 9.95714C76.2368 9.82854 76.5306 9.76424 76.8474 9.76424C77.5076 9.76424 78.0423 9.98066 78.4516 10.4135C78.8642 10.8432 79.0705 11.4783 79.0705 12.3189C79.0705 13.1155 78.8675 13.7773 78.4615 14.3042C78.0555 14.8312 77.4927 15.0946 76.7732 15.0946C76.3705 15.0946 76.0305 15.0021 75.7532 14.817C75.5882 14.7073 75.4116 14.5316 75.2234 14.2901V14.9347H74.4015V8ZM76.7187 14.3466C77.2006 14.3466 77.5604 14.1647 77.7981 13.8008C78.039 13.437 78.1595 12.9571 78.1595 12.3612C78.1595 11.8312 78.039 11.3921 77.7981 11.0439C77.5604 10.6958 77.2089 10.5217 76.7435 10.5217C76.3375 10.5217 75.981 10.6644 75.674 10.9498C75.3703 11.2352 75.2185 11.7057 75.2185 12.3612C75.2185 12.8348 75.2812 13.219 75.4066 13.5139C75.641 14.069 76.0784 14.3466 76.7187 14.3466Z" fill="white"/>
        <path d="M83.3262 9.89597H84.3115C84.1861 10.219 83.9071 10.9561 83.4747 12.1072C83.1513 12.9728 82.8806 13.6785 82.6627 14.2243C82.1478 15.5102 81.7847 16.2943 81.5735 16.5766C81.3622 16.8589 80.9991 17 80.4842 17C80.3588 17 80.2614 16.9953 80.1921 16.9859C80.1261 16.9765 80.0435 16.9592 79.9445 16.9341V16.1626C80.0997 16.2033 80.2119 16.2284 80.2812 16.2378C80.3505 16.2473 80.4116 16.252 80.4644 16.252C80.6294 16.252 80.7499 16.2253 80.8258 16.172C80.9051 16.1218 80.9711 16.0591 81.0239 15.9838C81.0404 15.9587 81.0998 15.8301 81.2021 15.598C81.3045 15.3659 81.3787 15.1934 81.4249 15.0805L79.4643 9.89597H80.4743L81.8953 13.9984L83.3262 9.89597Z" fill="white"/>
        <path d="M87.7033 8H88.5945V10.5781C88.8057 10.3241 88.9955 10.1453 89.1639 10.0418C89.4511 9.86304 89.8092 9.77365 90.2383 9.77365C91.0074 9.77365 91.5289 10.0293 91.8029 10.5405C91.9514 10.8197 92.0257 11.207 92.0257 11.7026V14.9347H91.1097V11.759C91.1097 11.3889 91.0602 11.1176 90.9612 10.9451C90.7994 10.6691 90.4958 10.5311 90.0502 10.5311C89.6805 10.5311 89.3454 10.6519 89.0451 10.8934C88.7447 11.1349 88.5945 11.5912 88.5945 12.2624V14.9347H87.7033V8Z" fill="white"/>
        <path d="M94.1375 9.89597V13.241C94.1375 13.4982 94.1804 13.7083 94.2662 13.8714C94.4246 14.1725 94.7201 14.3231 95.1525 14.3231C95.773 14.3231 96.1955 14.0596 96.42 13.5327C96.5421 13.2504 96.6032 12.863 96.6032 12.3706V9.89597H97.4944V14.9347H96.6527L96.6626 14.1913C96.5471 14.3826 96.4035 14.5442 96.2318 14.6759C95.8919 14.9394 95.4793 15.0711 94.994 15.0711C94.2381 15.0711 93.7232 14.8312 93.4493 14.3513C93.3007 14.0941 93.2265 13.7507 93.2265 13.321V9.89597H94.1375Z" fill="white"/>
        <path d="M98.7892 9.89597H99.6706V10.6111C99.8818 10.3633 100.073 10.183 100.245 10.07C100.539 9.87872 100.872 9.78306 101.245 9.78306C101.668 9.78306 102.008 9.88186 102.265 10.0795C102.41 10.1924 102.542 10.3586 102.661 10.5781C102.859 10.3084 103.092 10.1093 103.359 9.98066C103.627 9.84893 103.927 9.78306 104.26 9.78306C104.973 9.78306 105.459 10.0277 105.716 10.517C105.855 10.7804 105.924 11.1349 105.924 11.5802V14.9347H104.998V11.4344C104.998 11.0988 104.909 10.8683 104.731 10.7428C104.556 10.6174 104.341 10.5546 104.087 10.5546C103.737 10.5546 103.435 10.666 103.181 10.8887C102.93 11.1113 102.805 11.483 102.805 12.0037V14.9347H101.899V11.6461C101.899 11.3042 101.856 11.0549 101.77 10.8981C101.635 10.6628 101.382 10.5452 101.012 10.5452C100.676 10.5452 100.369 10.6691 100.091 10.9169C99.8174 11.1647 99.6805 11.6132 99.6805 12.2624V14.9347H98.7892V9.89597Z" fill="white"/>
        <path d="M107.818 13.5938C107.818 13.8385 107.912 14.0314 108.1 14.1725C108.288 14.3136 108.511 14.3842 108.769 14.3842C109.082 14.3842 109.386 14.3152 109.68 14.1772C110.175 13.9482 110.422 13.5734 110.422 13.0528V12.3706C110.313 12.4365 110.173 12.4914 110.001 12.5353C109.83 12.5792 109.661 12.6106 109.496 12.6294L108.957 12.6952C108.633 12.736 108.391 12.8003 108.229 12.8881C107.955 13.0355 107.818 13.2708 107.818 13.5938ZM109.977 11.8813C110.181 11.8562 110.318 11.7747 110.388 11.6367C110.427 11.5614 110.447 11.4532 110.447 11.3121C110.447 11.0235 110.338 10.815 110.12 10.6864C109.906 10.5546 109.597 10.4888 109.194 10.4888C108.729 10.4888 108.399 10.6079 108.204 10.8463C108.095 10.978 108.024 11.1741 107.991 11.4344H107.159C107.176 10.8134 107.387 10.3821 107.793 10.1406C108.202 9.89598 108.676 9.77365 109.214 9.77365C109.838 9.77365 110.345 9.88657 110.734 10.1124C111.12 10.3382 111.313 10.6895 111.313 11.1662V14.069C111.313 14.1568 111.332 14.2274 111.368 14.2807C111.408 14.334 111.488 14.3607 111.611 14.3607C111.65 14.3607 111.695 14.3591 111.744 14.356C111.794 14.3497 111.847 14.3419 111.903 14.3325V14.9582C111.764 14.9958 111.658 15.0193 111.586 15.0288C111.513 15.0382 111.414 15.0429 111.289 15.0429C110.982 15.0429 110.759 14.9394 110.62 14.7324C110.548 14.6226 110.496 14.4673 110.467 14.2666C110.285 14.4924 110.024 14.6884 109.684 14.8547C109.345 15.0209 108.97 15.104 108.561 15.104C108.069 15.104 107.666 14.9629 107.352 14.6806C107.042 14.3952 106.887 14.0392 106.887 13.6127C106.887 13.1453 107.041 12.7831 107.348 12.5259C107.654 12.2687 108.057 12.1103 108.556 12.0507L109.977 11.8813Z" fill="white"/>
        <path d="M112.678 9.89597H113.524V10.6111C113.775 10.3163 114.041 10.1045 114.321 9.97595C114.602 9.84736 114.914 9.78306 115.257 9.78306C116.01 9.78306 116.518 10.0324 116.782 10.5311C116.927 10.804 117 11.1945 117 11.7026V14.9347H116.094V11.759C116.094 11.4516 116.046 11.2039 115.95 11.0157C115.792 10.702 115.505 10.5452 115.089 10.5452C114.878 10.5452 114.704 10.5656 114.569 10.6064C114.325 10.6754 114.11 10.8134 113.925 11.0204C113.777 11.1866 113.679 11.3591 113.633 11.5379C113.59 11.7135 113.569 11.966 113.569 12.2953V14.9347H112.678V9.89597Z" fill="white"/>
    </svg>
</div>

<!-- <div class="sharethis-inline-reaction-buttons"></div>
<style>
  #st-1 .st-btn > svg
  {
    width:28px;
  }
</style> -->
<hr></hr>
<center><p>If you liked it, please share it with your friends and help this blog to grow!</p></center>
<div class="elfsight-app-c37f9300-422e-4d9d-a637-20a2cd5ddb06"></div>



    

<!-- <div class="sharethis-inline-share-buttons"></div> -->
<!-- <center><div class="addthis_inline_share_toolbox"></div></center> -->
<br>
<br>


    
        <script src="https://utteranc.es/client.js"
                repo="Akash-Sharma-1/Utteranc-Comments-Repo-Blog"
                issue-term="pathname"
                theme="github-light"
                crossorigin="anonymous"
                async>
        </script>
    


<script>
    // Add anchors to headers.
    anchors.add();

    // Show images in fancybox.
    $("p img").each(function() {
        $(this).wrapAll('<a data-fancybox="images" data-caption="' + this.alt + '" href="' + this.src + '"></a>');
    });
    $('[data-fancybox="images"]').fancybox({
        transitionEffect: "slide",

        // Support for retina displays.
        afterLoad : function(instance, current) {
            var pixelRatio = window.devicePixelRatio || 1;

            if ( pixelRatio > 1.5 ) {
                current.width  = current.width  / pixelRatio;
                current.height = current.height / pixelRatio;
            }
        }
    });
</script>


<script>
    // Show sidebar on top in mobile devices.
    $(".content").addClass("order-last order-md-first");
    $(".sidebar").addClass("order-first order-md-last");
</script>

</div>
<div class="sidebar col-md-3 mt-2 mt-md-0">
    
    <!-- https://github.com/christian-fei/Simple-Jekyll-Search -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.7.2/simple-jekyll-search.min.js" integrity="sha256-DsgE6Y6cI5eAZoTg8tJ5VBxFs+rnL8smPwt/u8tyAYo=" crossorigin="anonymous"></script>

<div id="search-box" class="mb-2">
    <input class="form-control" type="text" placeholder="Search" aria-label="Search">
    <ul class="mb-0"></ul>
</div>

<script type="text/javascript">
    var search_box = document.getElementById("search-box");
    SimpleJekyllSearch({
        searchInput: search_box.getElementsByTagName("input")[0],
        resultsContainer: search_box.getElementsByTagName("ul")[0],
        json: '/blog/assets/search.json',
        searchResultTemplate: '<li><a href="{url}">{title}</a></li>',
        noResultsText: 'No results found',
        fuzzy: false
    });
</script>

    <!-- https://github.com/tscanlin/tocbot/ -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.7.0/tocbot.css" integrity="sha256-XUbSD3m+xLW27W/mp9kMn+fI9AU2MoBkiMMSVVYWI+o=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.7.0/tocbot.min.js" integrity="sha256-5K/04vUxP94HmQ8f18TgufdyqE69WDJRMNga0JLhAtE=" crossorigin="anonymous"></script>

<div class="toc-container mb-2">
    <p class="name">Table of Contents</p>
    <div class="toc py-2 pr-2 mb-2"></div>
</div>

<script>
    // Show TOC if the article contains any heading.
    if ($("article h1, article  h2, article h3, article h4, article h5, article h6").length == 0) {
        $(".toc-container").addClass("d-none");
    }

    tocbot.init({
        // Where to render the table of contents.
        tocSelector: ".toc",
        // Where to grab the headings to build the table of contents.
        contentSelector: "article",
        // Which headings to grab inside of the contentSelector element.
        headingSelector: "h1, h2, h3, h4, h5, h6",
        collapseDepth: 6,
    });
</script>






</div>


    </div>
</div>

        <footer>
    <div class="container">
        <hr class="my-0">
        <div class="row align-items-center text-muted my-3">
            <div class="col">
                <span>© 2023 Akash Sharma</span>
                <a href="/blog/feed.xml"><span class="feed iconify" data-icon="foundation:rss"></span></a>
            </div>
            <div class="col-auto text-center px-0">
                    <a href="https://akash-sharma-1.github.io/"><span class="github text-center text-muted iconify" data-icon="foundation:web"></span></a>
                    <a href="https://github.com/Akash-Sharma-1"><span class="github text-center text-muted iconify" data-icon="foundation:social-github"></span></a>
                    <a href="https://twitter.com/AkashTheGreat_1"><span class="github text-center text-muted iconify" data-icon="foundation:social-twitter"></span></a>
                    <a href="https://www.linkedin.com/in/akash-sharma-246b67165"><span class="github text-center text-muted iconify" data-icon="foundation:social-linkedin"></span></a>
                    <a href="mailto:akashthegreatlegend@gmail.com"><span class="github text-center text-muted iconify" data-icon="foundation:mail"></span></a>
            </div>
            <div class="col text-right">
                <a href="javascript:$('html,body').animate({ scrollTop: 0 }, 'slow');">TOP</a>
            </div>

        </div>
    </div>
</footer>

    </body>
</html>
